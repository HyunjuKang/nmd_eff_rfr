We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.

### W1: Addressing novelty

Thank you for raising the important question. We acknowledge that our method is not the first to implement Monte Carlo Tree Search (MCTS) in the graph XAI (Explainable Artificial Intelligence) domain. Prior works, such as RationaleRL [1] and SubgraphX [2], have utilized MCTS in their problem settings, each with a reward function tailored to their specific objectives. However, applying existing MCTS frameworks directly to unsupervised node learning models is challenging. The difficulty lies in defining suitable rewards and managing the increased uncertainty in exploring the graph space.

To mitigate the challenges, we propose a new procedure that is different from the existing MCTS framework with respect to five key aspects. The distinctions between our model and SubgraphX are summarized in the table below.
First, our $Importance$ function shown in Equation (1) of our main text quantifies the change of top-k neighbors in the embedding space after perturbation, bringing up the benefits of recognizing the significant influence of important subgraph $G_s$ in related downstream tasks. Thus, our contribution not only relies on developing the proper design of MCTS for counterfactual explanations but also sheds light on explaining unsupervised node representation learning. For clarification, we describe the difference between SubgraphX and UNR-Explainer regarding to the design of MCTS as below:

| Design criteria        | SubgraphX          |   UNR-Explainer                |
|------------------------|--------------------|--------------------------------|
| Target model           | supervised models  | unsupervised models            |
| Reward function        | Shapley value      | Importance function in Eq. (1) |
| Action                 | to prune           | to add                         |
| Selection              | argmax             | random walk with restart       |
| Expansion              | without sampling   | with sampling                  |
| Action value           | mean               | max                            |

Consequently, our design of Monte Carlo Tree Search (MCTS) in UNR-Explainer reduces the computational time by 97.37\% [(4.64 - 176.3) ÷ 176.3] ×100 and performance increased by 1.9\% ([(0.96 - 0.42) ÷ 0.942] ×100) in the Table 4 of our paper, compared to SubgraphX-1. The significant improvement in the computational cost indicates the emphasis on MCTS design to tailor its problem settings. To avoid costly and unnecessary exploration, we propose a suitable MCTS framework for counterfactual explanations in unsupervised settings. Additionally, the new selection strategy and reward functions are theoretically analyzed with respect to the bounds of value and expressiveness, respectively.

 **Reference** 
 
[1] Jin, Wengong, Regina Barzilay, and Tommi Jaakkola. "Multi-objective molecule generation using interpretable substructures." In International conference on machine learning, pp. 4849-4859. PMLR, 2020.

[2] Yuan, Hao, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. "On explainability of graph neural networks via subgraph explorations." In International conference on machine learning, pp. 12241-12252. PMLR, 2021.

### W2: Clarity of Figure 1

Thank you for your thoughtful comments. For brevity, we have simplified the counterfactual explanations in Figure-1 (b) and (c). As indicated, when edges near the node of interest are perturbed, the neighboring nodes' positions are affected. In contrast, nodes that are distant from the node of interest, but share structural similarities and similar feature distributions, will maintain their original embeddings unchanged. We have updated the figure to better illustrate this distinction in a revised version of our paper.

### W3 & Q2: Addressing motivation

Thank you for pointing this out. UNR-Explainer is designed to generate counterfactual explanations in unsupervised learning contexts without relying on labels, contrasting with XAI methods in supervised learning. We highlight the limitations of existing methods when applied to unsupervised learning:

- CF-GNNExplainer [1] as a pioneer of counterfactual explanation for the graph domain enhances explainability. Its loss function is formulated as 
 $L = L_{pred}(v, \bar{v}| f, g) + βL_{dist}(v,  \bar{v} | d)$, where $v$ is the original node and $\bar{v}$ is generated by the CF model. In this function, $f$ represents a predicted class label, and $g$ is the counterfactual (CF) model generating $\bar{v}$. The term $L_{pred}$ denotes a counterfactual loss ensuring $f(v) \neq f(\bar{v})$, while $L_{dist}$ quantified the distance between the input graph and the found explanation. However, as indicated in Definition 2 of our paper, the condition $f(v) \neq f(\bar{v})$ in supervised settings can be analogous to $emb_{v} \neq emb_{v}^{\prime}$ in unsupervised learning. Yet, $emb_{v} \neq emb_{v}^{\prime}$ alone provides limited information for determining the importance of each edge.  To address this and aid in generating beneficial explanations for related downstream tasks, we introduce a novel $Importance$ measure for counterfactual explanation in unsupervised node representation learning.

- RCExplainer [2] tackles the robustness of counterfactual explanations by leveraging the decision boundaries of a GNN in supervised settings. However, the decision region in [2] heavily relies on class labels, making it unsuitable for unsupervised settings where class labels are absent or when no specific downstream tasks are defined.

- CF2 [3] combines factual and counterfactual explanations to address the limitations of each approach. However, the process for generating explanations in CF2, as defined in Equations (10), (11), and (12) in [3], also depends on class labels for graph/node classification. This reliance makes direct application to unsupervised settings challenging.

In conclusion, UNR-Explainer provides insights into counterfactual explanations in unsupervised settings without relying on class labels. It offers the advantage of describing potential impacts on related downstream tasks, such as link prediction and clustering.

 **Reference**
 
[1] Lucic, Ana, Maartje A. Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri. "Cf-gnnexplainer: Counterfactual explanations for graph neural networks." In International Conference on Artificial Intelligence and Statistics*, pp. 4499-4511. PMLR, 2022.

[2] Bajaj, Mohit, Lingyang Chu, Zi Yu Xue, Jian Pei, Lanjun Wang, Peter Cho-Ho Lam, and Yong Zhang. "Robust counterfactual explanations on graph neural networks."Advances in Neural Information Processing Systems 34 (2021): 5644-5655.

[3] Tan, Juntao, Shijie Geng, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Yunqi Li, and Yongfeng Zhang. "Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning." InProceedings of the ACM Web Conference 2022*, pp. 1018-1027. 2022.

### W4 \& Q3: Addressing real-world scenario

Thank you for your invaluable question. We address the potential risks inherent in explainability techniques, particularly those involving modification methods like adding new nodes or edges, in real-world scenarios. In the context of social network graphs, an explanation graph generated by allowing the addition of nodes to a subgraph (used for explaining link prediction or clustering) might include nodes that are not actually observable. For example, it may suggest connections with individuals who have never met, rendering the explanation invalid for the end-user. Similarly, in citation graphs, if the explanation for link prediction includes a non-existent citation subgraph, it becomes challenging to interpret. When learning from the molecule datasets for drug discovery, permitting the addition of numerous nodes can lead to significant deviations from [chemical valency rules](https://en.wikipedia.org/wiki/Valence_(chemistry)). Such alterations could render the explanations not only inaccurate but also impractical for practical applications. To circumvent these issues, our approach emphasizes the removal of important connections to provide counterfactual explanations. This method ensures that the explanations remain grounded in the actual structure of the dataset, thereby enhancing their validity and applicability.


### W5: Rectifying minor error

Thank you for your thoughtful comment. We revised this issue in the current version of our paper. 

### Q1: Rationale for MCTS

Thank you for raising the important question. Our approach offers several advantages over gradient-based or causal-based interpretable methods. Firstly, unlike gradient-based methods such as PG-Explainer, which attempt to ensure connectedness through a connected loss term but often fail to guarantee connected subgraphs, our MCTS-based framework reliably identifies exploratory subgraphs that are connected. This is because the additional regularization term in gradient-based methods does not consistently lead to connected subgraphs. Secondly, our objective function is not a differential equation, setting it apart from gradient-based methods. While SubgraphX employs the Shapley value for the importance measurement in its MCTS framework, our $Importance$ measure quantifies the change in the k-nearest neighbor nodes after perturbation, making our MCTS-based approach more suitable for exploring significant subgraphs in unsupervised settings.
As for causal-based methods like GEM and OrphicX, they rely on structural causal models (SCM), which are predominantly based on the assumption of existing class labels. Developing a causal-based model for explaining embedding vectors in unsupervised settings is an intriguing area for future research. However, in the absence of external SCM models, our method solely relies on our $Importance$ function to provide explanations in unsupervised settings.

**Reference**

[1] Luo, Dongsheng, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. "Parameterized explainer for graph neural network." Advances in neural information processing systems 33 (2020): 19620-19631.

[2] Yuan, Hao, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. "On explainability of graph neural networks via subgraph explorations." In International conference on machine learning, pp. 12241-12252. PMLR, 2021.

[3] Lin, Wanyu, Hao Lan, and Baochun Li. "Generative causal explanations for graph neural networks." In International Conference on Machine Learning, pp. 6666-6679. PMLR, 2021.

[4] Lin, Wanyu, Hao Lan, Hao Wang, and Baochun Li. "Orphicx: A causality-inspired latent variable model for interpreting graph neural networks." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13729-13738. 2022.


We express our sincere gratitude for  the opportunity to improve our paper based on your valuable feedback. The revisions made in response aim to comprehensively address the raised concerns.
