We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.

### W1 \& Q1: Addressing scalability 

Thank you for raising this important point. Scalability is an important factor to be considered particularly in real applications on very large graphs. As a matter of time complexity, UNR-Explainer has a time complexity of $\(O(t \cdot n \cdot |V|)\)$, where $\(t\)$ is the number of iterations, $\(n\)$ is the number of nodes in the search tree, and $\(|V|\)$ is the number of nodes in the input graph. In the worst case, if $\(n\)$ approaches $\(|V|\)$, the complexity could become approximately $\(O({|V|}^2)\)$. To circumvent this situation, there are two strategies: establishing stopping criteria and setting an upper bound for $\(n\)$. Firstly, $\(t\)$ is set to 1,000 as a stopping criterion, or the traversal is stopped when a case meeting $\(Importance = 1.0\)$ is found. Secondly, the number of nodes in the traversal graph is limited to a constant value; in this paper, we set it to 20. Moreover, in every iteration, we sample at most 3 nodes for the expansion step, which helps avoid the exponential expansion of $\(n\)$. For these reasons, the time complexity of UNR-Explainer is approximately $\(O(|V|)\)$, because both $\(t\)$ and $\(n\)$ could be regarded as constants. While most gradient-based methods (e.g., PGExplainer [1]) have the time complexity of $\(O(|E|)\)$, where $\(|E|\)$ is the number of edges in the input graph, the complexity of UNR-Explainer could be more efficient than these methods, especially when the input graph is dense.

**Reference**

[1] Luo, Dongsheng, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. "Parameterized explainer for graph neural network." Advances in neural information processing systems 33 (2020): 19620-19631.

### W2: Providing overview of UNR-Explainer

Thank you for the suggestion. We attached the figure describing an overview of UNR-Explainer in a revised version of our paper and attached [link](https://anonymous.4open.science/r/unr0929/figure_overview.jpg).
### Q2: Addressing sensitivity of perturbation effect
Thanks for your valuable question. The sensitivity of explanations to perturbations can vary based on the nature of the data, 
However, as observed in additional experiments following [link](https://anonymous.4open.science/r/unr0929/perturbation_param.jpg), less perturbation leads to less $Improtance$ as the counterfactual results. CiteSeer has fewer connections with an average degree of around 2, while Cora has more connections with an average degree of around 3. However, consistent findings indicate that, across different datasets, perturbation carries less importance when the magnitude of perturbation is lower. Thus, we recommend setting a perturbation as 0.0 which is equal to removing edges on most of the input graph to align well in the counterfactual context.

### Q3-1: Addressing analysis of explanations by models as GraphSAGE vs DGI
Thanks for brining up this question. UNR-Explainer under the different node representation models such as GraphSAGE and DGI have different sizes of explanation. Explanations under the model as GraphSAGE have an average number of nodes equal to 4.5 and edges equal to 3.8, respectively. On the other hand, Explanations under the model as DGI have an average number of nodes equal to 3.9 and edges equal to 3.1, respectively. Explanations under GraphSAGE include more distance nodes and more connections in the input graph than DGI in general. We attached the visualization of explanations by UNR-Explainer following this [link](https://anonymous.4open.science/r/unr0929/cora-gs-dgi.png). In Case 1, Explanations under GraphSAGE and DGI have in common, and in Case 2, the overall structure of explanations is identical but the explanation under GraphSAGE has an extra node in 2-hop distance of the target node. This tendency is observed more severely in Case 3. We assume that one of the major causes may relate to the way of defining positive and negative samples for training. GraphSAGE treats nearby nodes on a fixed-length random walk (as [10, 10] in our setting) as positive samples, otherwise as the negatives. On the other side, DGI defines a corruption function that randomly permutates the features of nodes while fixing the edge index to generate negative samples. The comparatively wider range of positive samples in GraphSAGE may affect the generating of explanations with more distance nodes and edges by UNR-Explainer.

### Q3-2: Adapting UNR-Explainer to explain generative models 

Thanks for your invaluable question. UNR-Explainer can be applied to explain the Encoder of the GAE-based model. Graph Auto Encoder (GAE), i.e. VGAE [1] (if I interpret GraphGAE as GAE correctly) and S2GAE [2] are one of the main streams in generation-based unsupervised learning and self-supervised learning on graphs, respectively. GAE and S2GAE have two main parts; the Encoder as graph neural networks outputs latent representation and the Decoder reconstructs the input graph using previously learned representation. Latent representation containing implicit information has been utilized to serve downstream tasks such as node classification and graph classification, showing superior performance. Thus, explaining those models' demands in real-world applications. Since learned representation from encoders of GAE is the core of output, UNR-Explainer has the potential to explain the encoder of the GAE-based model. This is due to two reason mainly that UNR-Explainer provide node-level explanations and it aligns well with the encoder of VGAE and S2GAE because the encoderâ€™s output represents embedding usually in node-level. Additionally, UNR-Explainer provides explanations without class labels so that it possibly expands the self-supervised learning model. To develop this idea, we first need to define the explanations for generation-based GAE and design proper experimental settings, particularly about evaluation metrics. Furthermore, it is essential to consider the process of each model, since VGAE [1] and S2GAE [2] have different approaches to the generation of graphs. In VGAE [1], we control the randomness of the reparameterization trick properly to examine the perturbation effect. S2GAE [2] applies perturbation of edges to apply the edge-masking method and direction-aware graph masking so that these perturbation methods conflict with the perturbation process in UNR-Explainer, requiring additional processes. Furthermore, since the models to be explained in UNR-Explainer have the only Encoder part to project the nodes in Euclidian space, the Decoder part of generative models requires further decisions to encompass. To our knowledge, explainability for generative models on graphs has not been studied yet. The question lights an interesting direction for our future research.

**Reference**

[1] Kipf, Thomas N., and Max Welling. "Variational graph auto-encoders." arXiv preprint arXiv:1611.07308 (2016).

[2] Tan, Qiaoyu, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, and Xia Hu. "S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking." In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pp. 787-795. 2023.
