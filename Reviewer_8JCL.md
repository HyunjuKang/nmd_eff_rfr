We sincerely appreciate your valuable feedback for reviewing this paper. For the potential concerns you bring up, we would like to address them as follows.

### W1 \& Q1: Addressing Scalability 

Thank you for raising this important point. Scalability is an important factor to be considered particularly in real applications on very large graphs. While the total time complexity of UNR-Explainer is $O(t \cdot n \cdot |V|)$ in the worst case, there are several ways to escape the worst case in our MCTS settings. First, $t$ is limited to prevent an infinite loop when $Importance$ does not achieve 1.0. Secondly, $n$ as the number of nodes in the search tree is also constrained. In our MCTS setting, we implement a sampling method when a current tree expands child nodes. It potentially prevents exhausted cost for exploration especially when the input is extremely dense. The sampling in the expansion would be an answer for memory constraints on the assumption that UNR-Explaienr employs in dense graphs. Additionally, we choose the number of child nodes as the average degree of the input graph in the Expansion step. Please refer to section 4.2 - expansion for details of the process in our paper. Last, $|V|$ are not considered in all of the nodes in the input graph, but in relative close-hop nodes. In practice, the time complexity of UNR should be linear. Additionally, we tailor the design of MCTS carefully to reduce the computational time by 97.37\% as ((4.64 - 176.3) ÷ 176.3)×100 and performance increased by 1.9\% as ((0.96 - 0.42) ÷ 0.942) ×100 compared to SubgraphX-1 in Table 4.

### W2: Providing overview of UNR-Explainer

Thank you for the suggestion. We attached the figure describing an overview of UNR-Explainer in a revised version of our paper and attached [link](https://anonymous.4open.science/r/unr0929/figure_overview.jpg)

### Q2: Addressing Sensitivity of Perturbation effect
Thanks for your valuable question. The sensitivity of explanations to perturbations can vary based on the nature of the data, 
However, as observed in additional experiments following [link](https://anonymous.4open.science/r/unr0929/perturbation_param.jpg), less perturbation leads to less $Improtance$ as the counterfactual results. CiteSeer has fewer connections with an average degree of around 2, while Cora has more connections with an average degree of around 3. However, consistent findings indicate that, across different datasets, perturbation carries less importance when the magnitude of perturbation is lower. Thus, we recommend setting a perturbation as 0.0 which is equal to removing edges on most of the input graph to align well in the counterfactual context.

### Q3-1: Addressing analysis of explanations by models as GraphSAGE vs DGI
Thanks for brining up this question. UNR-Explainer under the different node representation models such as GraphSAGE and DGI have different sizes of explanation. Explanations under the model as GraphSAGE have an average number of nodes equal to 4.5 and edges equal to 3.8, respectively. On the other hand, Explanations under the model as DGI have an average number of nodes equal to 3.9 and edges equal to 3.1, respectively. Explanations under GraphSAGE include more distance nodes and more connections in the input graph than DGI in general. We attached the visualization of explanations by UNR-Explainer following this [link](https://anonymous.4open.science/r/unr0929/cora-gs-dgi.png). In Case 1, Explanations under GraphSAGE and DGI have in common, and in Case 2, the overall structure of explanations is identical but the explanation under GraphSAGE has an extra node in 2-hop distance of the target node. This tendency is observed more severely in Case 3. We assume that one of the major causes may relate to the way of defining positive and negative samples for training. GraphSAGE treats nearby nodes on a fixed-length random walk (as [10, 10] in our setting) as positive samples, otherwise as the negatives. On the other side, DGI defines a corruption function that randomly permutates the features of nodes while fixing the edge index to generate negative samples. The comparatively wider range of positive samples in GraphSAGE may affect the generating of explanations with more distance nodes and edges by UNR-Explainer.

### Q3-2: Adapting UNR-Explainer to explain generative models 

Thanks for your invaluable question. UNR-Explainer can be applied to explain the Encoder of the GAE-based model. Graph Auto Encoder (GAE), i.e. VGAE [1] (if I interpret GraphGAE as GAE correctly) and S2GAE [2] are one of the main streams in generation-based unsupervised learning and self-supervised learning on graphs, respectively. GAE and S2GAE have two main parts; the Encoder as graph neural networks outputs latent representation and the Decoder reconstructs the input graph using previously learned representation. Latent representation containing implicit information has been utilized to serve downstream tasks such as node classification and graph classification, showing superior performance. Thus, explaining those models' demands in real-world applications. Since learned representation from encoders of GAE is the core of output, UNR-Explainer has the potential to explain the encoder of the GAE-based model. This is due to two reason mainly that UNR-Explainer provide node-level explanations and it aligns well with the encoder of VGAE and S2GAE because the encoder’s output represents embedding usually in node-level. Additionally, UNR-Explainer provides explanations without class labels so that it possibly expands the self-supervised learning model. To develop this idea, we first need to define the explanations for generation-based GAE and design proper experimental settings, particularly about evaluation metrics. Furthermore, it is essential to consider the process of each model, since VGAE [1] and S2GAE [2] have different approaches to the generation of graphs. In VGAE [1], we control the randomness of the reparameterization trick properly to examine the perturbation effect. S2GAE [2] applies perturbation of edges to apply the edge-masking method and direction-aware graph masking so that these perturbation methods conflict with the perturbation process in UNR-Explainer, requiring additional processes. Furthermore, since the models to be explained in UNR-Explainer have the only Encoder part to project the nodes in Euclidian space, the Decoder part of generative models is in need of further decisions to encompass. To our knowledge, explainability for generative models on graphs has not been studied yet. The question raised lights up an interesting direction for our future research

**Reference**

[1] Kipf, Thomas N., and Max Welling. "Variational graph auto-encoders." arXiv preprint arXiv:1611.07308 (2016).

[2] Tan, Qiaoyu, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, and Xia Hu. "S2GAE: Self-Supervised Graph Autoencoders are Generalizable Learners with Graph Masking." In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pp. 787-795. 2023.
